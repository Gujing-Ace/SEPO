{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7803014bf194ab3bad0ce7b75097020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['ds_name', 'image', 'question', 'chosen', 'rejected', 'origin_dataset', 'origin_split', 'idx', 'image_path'],\n",
      "        num_rows: 83132\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_root = \"./data/RLAIF-V-Dataset\"\n",
    "data_file = [f'{data_root}/RLAIF-V-Dataset_{i:03d}.parquet' for i in range(14)]\n",
    "data = load_dataset('parquet', data_files=data_file)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'LCS-558K': 15956, 'COCO': 15199, 'OK-VQA': 14802, 'VQAv2': 12942, 'GQA': 5411, 'TextVQA': 4740, 'OCR-VQA': 3025, 'sharegpt4v-wikiart': 1972, 'sharegpt4v-textvqa': 1966, 'sharegpt4v-web-landmark': 1918, 'sharegpt4v-web-celebrity': 1895, 'MovieNet': 1131, 'ART500K': 1096, 'Google-Landmark': 1079})\n",
      "==================================\n",
      "OK-VQA: 17.81%\n",
      "TextVQA: 5.70%\n",
      "COCO: 18.28%\n",
      "LCS-558K: 19.19%\n",
      "sharegpt4v-wikiart: 2.37%\n",
      "VQAv2: 15.57%\n",
      "sharegpt4v-textvqa: 2.36%\n",
      "sharegpt4v-web-celebrity: 2.28%\n",
      "sharegpt4v-web-landmark: 2.31%\n",
      "GQA: 6.51%\n",
      "OCR-VQA: 3.64%\n",
      "MovieNet: 1.36%\n",
      "Google-Landmark: 1.30%\n",
      "ART500K: 1.32%\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "origin_dataset = data['train']['origin_dataset']\n",
    "counts = Counter(origin_dataset)\n",
    "print(counts)\n",
    "total_len = len(origin_dataset)\n",
    "propotions = {ds: num/total_len for ds, num in counts.items()}\n",
    "print('==================================')\n",
    "for ds, num in propotions.items():\n",
    "    print(f'{ds}: {num:.2%}')\n",
    "print('==================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "Sampled dataset proportions: total count: 2500\n",
      "TextVQA: 5.68%\n",
      "OCR-VQA: 3.64%\n",
      "sharegpt4v-textvqa: 2.36%\n",
      "GQA: 6.52%\n",
      "sharegpt4v-wikiart: 2.40%\n",
      "LCS-558K: 19.16%\n",
      "COCO: 18.28%\n",
      "VQAv2: 15.60%\n",
      "MovieNet: 1.36%\n",
      "OK-VQA: 17.80%\n",
      "sharegpt4v-web-celebrity: 2.28%\n",
      "Google-Landmark: 1.32%\n",
      "ART500K: 1.32%\n",
      "sharegpt4v-web-landmark: 2.28%\n",
      "=====================================\n",
      "=====================================\n",
      "Sampled dataset proportions: total count: 2500\n",
      "ART500K: 1.32%\n",
      "VQAv2: 15.56%\n",
      "OCR-VQA: 3.64%\n",
      "COCO: 18.28%\n",
      "OK-VQA: 17.84%\n",
      "GQA: 6.48%\n",
      "LCS-558K: 19.20%\n",
      "sharegpt4v-web-celebrity: 2.28%\n",
      "sharegpt4v-wikiart: 2.36%\n",
      "MovieNet: 1.36%\n",
      "TextVQA: 5.72%\n",
      "sharegpt4v-textvqa: 2.36%\n",
      "Google-Landmark: 1.28%\n",
      "sharegpt4v-web-landmark: 2.32%\n",
      "=====================================\n",
      "=====================================\n",
      "Sampled dataset proportions: total count: 2500\n",
      "LCS-558K: 19.20%\n",
      "GQA: 6.52%\n",
      "OK-VQA: 17.80%\n",
      "VQAv2: 15.56%\n",
      "COCO: 18.28%\n",
      "sharegpt4v-wikiart: 2.36%\n",
      "ART500K: 1.32%\n",
      "sharegpt4v-web-celebrity: 2.28%\n",
      "OCR-VQA: 3.64%\n",
      "sharegpt4v-textvqa: 2.36%\n",
      "sharegpt4v-web-landmark: 2.32%\n",
      "Google-Landmark: 1.28%\n",
      "TextVQA: 5.72%\n",
      "MovieNet: 1.36%\n",
      "=====================================\n",
      "=====================================\n",
      "Sampled dataset proportions: total count: 2500\n",
      "ART500K: 1.32%\n",
      "VQAv2: 15.56%\n",
      "COCO: 18.28%\n",
      "OCR-VQA: 3.64%\n",
      "LCS-558K: 19.20%\n",
      "sharegpt4v-textvqa: 2.36%\n",
      "OK-VQA: 17.80%\n",
      "sharegpt4v-web-landmark: 2.32%\n",
      "Google-Landmark: 1.32%\n",
      "GQA: 6.52%\n",
      "TextVQA: 5.68%\n",
      "sharegpt4v-wikiart: 2.36%\n",
      "sharegpt4v-web-celebrity: 2.28%\n",
      "MovieNet: 1.36%\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "sample_num = 2500*4\n",
    "df = pd.DataFrame(data['train'])\n",
    "_, df_sampled0 = train_test_split(df, test_size=sample_num, stratify=df['origin_dataset'], random_state=42)\n",
    "df_sampled01, df_sampled02 = train_test_split(df_sampled0, test_size=sample_num//2, stratify=df_sampled0['origin_dataset'], random_state=42)\n",
    "df_sampled1, df_sampled2 = train_test_split(df_sampled01, test_size=sample_num//4, stratify=df_sampled01['origin_dataset'], random_state=42)\n",
    "df_sampled3, df_sampled4 = train_test_split(df_sampled02, test_size=sample_num//4, stratify=df_sampled02['origin_dataset'], random_state=42)\n",
    "\n",
    "for df_sampled in (df_sampled1, df_sampled2, df_sampled3, df_sampled4):\n",
    "    counts = Counter(df_sampled['origin_dataset'])\n",
    "    total_count = len(df_sampled['origin_dataset'])\n",
    "    proportions = {ds: count / total_count for ds, count in counts.items()}\n",
    "    print(\"=====================================\")\n",
    "    print(\"Sampled dataset proportions: total count:\", total_count)\n",
    "    for ds, proportion in proportions.items():\n",
    "        print(f\"{ds}: {proportion:.2%}\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ed08c128764927bd0cbae5d081f584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dd0611b639474bba243fdf621a8604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706170beac154880818dcd330dd99aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9024b3b93aa84a3081bf2169185953d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_paths = {\n",
    "    'df_sampled1': './data/RLAIF_Sample/subset1',\n",
    "    'df_sampled2': './data/RLAIF_Sample/subset2',\n",
    "    'df_sampled3': './data/RLAIF_Sample/subset3',\n",
    "    'df_sampled4': './data/RLAIF_Sample/subset4',\n",
    "}\n",
    "for path, df in zip(save_paths.keys(), [df_sampled1, df_sampled2, df_sampled3, df_sampled4]):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset.save_to_disk(save_paths[path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen2",
   "language": "python",
   "name": "qwen2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
